{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technical Report for Yelp Review Classification and Sentiment Analysis\n",
    "__________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "_____________________________________________________________________________________________\n",
    "\n",
    "Yelp is one of the most prolific and impactful crowd-sourced review platform on the internet. It has more impact on more restaurants than any other review based website or travel guide. IT can literally make or break a restuarant. As a former chef I can tell you that yelp impacts business. If you get a string of negstive(or low star) reviews it can affect business going forward. I personally know restaurants who have closed, partially due to very negative yelp reviews. Culling of the herd is good but the problem with yelp reviews is that even a 2 star review could technically be \"positive\" while a 5 star review gives you very little information to actually make your restaurant or food establishment better.\n",
    "\n",
    "40% of world population has internet connection today compared to 1% in 1995. Almost 3\n",
    "exabytes of data is created per day using internet. Storing huge amount of data and retrieving\n",
    "knowledge out of it is challenging task these days. Yelp publishes crowd sourced reviews about local businesses (Restaurants, Department Stores, Bars, Home-Local Services, Cafes, Automotive). It provides opportunity to business owners to improve their\n",
    "services and users to choose best business amongst available. For our purposes we will be using only Restaurant and Food establishment reviews.\n",
    "\n",
    "For the purpose of this project we will be using VADER for looking at sentiment anlysis for positive and negative words in reviews, as well as the overall positivity of the reviews themselves. I will be using Latent Dirichlet Allocation and Latent Sentiment Analysis to do topic modeling to try to find specific topics that are present in the entireity of the corpus. Next we will use Word2Vec to do further EDA on the semantic relationship between certain words. Finally I will be using a TF-IDF Vectorizer with Logistic Regression and a Linear Support Vector Classifier to traing a binary classification model on the yelp reviews to be able to predict either positive or negative reviews.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Data\n",
    "_____________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for this project is publicly available via the Kaggle competition for the yelp dataset. The files themselves are available in .json format so it was necessary to either read it in as json or convert them to .csv files, both of which I accomplished. There are multiple different files however for the purposes of my project I only used the Business and Review files. The data dictionary for the Review dataset is as follows:\n",
    "\n",
    "- Our review data contains 5996996 reviews, with the following information for each one:\n",
    "\n",
    "    1. business_id (ID of the business being reviewed)\n",
    "    2. date (Day the review was posted)\n",
    "    3. review_id (ID for the posted review)\n",
    "    4. stars (1–5 rating for the business)\n",
    "    5. text (Review text)\n",
    "    6. type (Type of text)\n",
    "    7. user_id (User’s id)\n",
    "    8. {cool / useful / funny} (Comments on the review, given by other users)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "______________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the original datasert having almost 6 million reviews it was necessary to seperate out all non-restaurant/food reviews from the reviews for other businesses that dont have any food related reviews. Without doing this we would be using data that has nothing to do with the problem we are trying to solve. I used the business_id column from both the review dataset as well as the business dataset to add names and states to the review data set. The original dataset contained reviews from the following metropolitan regions: Edinburgh (UK), Stuttgart (Germany), Montreal (Canada), Toronto (Canada), Pittsburgh , Charlotte, Champaign-Urbana, Phoenix, Las Vegas, Madison, and Cleveland. I wanted to use only reviews with English words to make the classification process easier on the computer, because of this I excluded businesses in Edinburgh, Stuttgart, and Montreal to eliminate the main sources of reviews written in a non-English language.\n",
    "\n",
    "Below is a histogram of the distribution of rating by star."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./visuals/eda_hist.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also plotted a seaborn facetgrid of the stars review length to see any trends. Interestingly the 5 star reviews tend to be much shorter than the less \"postive\" reviews. As a restaurant operator it would be very helpful if people where able to articulate more of the good AND bad for better service to the customer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./visuals/stars_text.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a box plot of the stars count in relation to text length for another view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./visuals/box_stars_text.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I looked at a box plot of our nex column \"is_positive\" to see further insight into text length vs. positivity. As expected the positive reviews tend to be much shorter on average than the negative reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./visuals/box_positive_text.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last basic EDA plot I looked at was a heatmap for the correlation between our numeric variables. Interestingly theres a strong positive correlation between the \"cool\" votes recieved by a reviw and the positivity. This makes logial sense, since most people dont tend to upvote very negative restuarant reviews. Also very interesting was the strong negative correlation in the other categories. Text lengths negatvie correlation follows along witht the theme we have seen in the graphs above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./visuals/heatmap.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP and Topic Modeling\n",
    "_____________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When dealing with text in Natural Language Processing(NLP) its important to follow some preprocessing steps. These allow the computer to more easily understand how words interact with one another and actually give \"sentiment\" knowledge to the computer. There are many different techinqes for this process but the flow chart is as follows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./visuals/nlp_flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preprocessing stage we use Spacey and Textacy to \"clean\" the text so thsat a computer can more easily understand it for tokenizing and lemmatizing. All punctation is removed along with URL's, hashtags and finally lowercasing all words. The tokenizing is the enxt step before vectorizing. In this step stop words(common english words that dont have \"statitstical\" value) are removed and each word in each review is seperated into a list for each \"document\" or review in our corpus(entireity of our text). Once all of our documents are self contained lists of seperated words we are then able to feed the tokenized text into a vectorizer that will convert the words into numbers. Statitsitcal modeling can only be done on numbers so this step is required to run machine learning models on text. \n",
    "\n",
    "As mentioned before we are using 2 different vectorizers, the first being TF-IDF for modeling. TF-IDF vectorizing results in a document-term sparse matrix containing the tf-idf frequency of the top 10,000 features across all documents in the corpus of documents. The second vectorizer is called Word2Vec, its special because it actually holds on to some of the sentiment of the words surrounding the given word, allowing for more exploratative EDA on the sematincs of the vectorizer. Although Word2Vec word embeddings can be used in modeling I choose to simply use it for EDA purposes and TF-IDF for modeling.\n",
    "\n",
    "Topic modeling was performed using Latent Semantic Allocation(LSA) as well as Latent Dirichlet Allocation just to see if theres any major differences in the topics they find. Due to computational memory restirctions I kept these models to 20 topics. After completeing both there didnt seem to be too much difference in topic modelings but I felt the LSA performed better with more accurate topics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VADER Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VADER(Valence Aware Dictionary for Senntiment Reasoning) is a sentiment analysis tool available in python that uses a combination of qualitative and quantitative methods to construct and empirically validate a gold-standard list of lexical features(along with their associated sentiment intensity measures) which are specifically attuned to sentiment in microblog-like  contexts. It then combine these lexical features with consideration for five general rules that embody grammatical and syntactical conventions for expressing and emphasizing sentiment intensity.\n",
    "\n",
    "Below are some quick glance examples of the \"scores\" from VADER on the full review text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Highest positive scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./visuals/vader_pos.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Highest Negative Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./visuals/vader_neg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Highest Neutral Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./visuals/vader_neu.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Highest Compound Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its follows in line with our assumption that more positive reviews are shorts and more negative reviews are longer when you look at the dataframes above. Its also interesting to note that the compound scores seemingly have the most information(text) in conjuction with positivity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Modeling - LSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our LSA I did a basic model with 100 topics. After doing so I wanted to plot the explained variance by our topics to see if there was an \"elbow\" that shows at what point the topic count stops being useful for explained variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./visuals/LSA_var.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above there is still a steady increase in the explained variance with more topics. With more computing power it would be possible to keep adding more topics, hence making the model more accurate however my computer couldnt take the strain! \n",
    "\n",
    "Next we use a termite graph to show the top 20 topics along with some unique words and how they fit into each topic. A termite plot can be interpreted as such: topics are on the x-axis at the top of the plot, with unique terms on the y-axis. Larger circles at a topic/term intersection mean that term had a heavier weight for that topic. Color coding shows the relative strength of terms across the highlighted topics; low numbers of highlighted terms for a topic indicate that the topic did not share many terms with the other selected topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./visuals/rest_termite_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we can look at some direct output of topics and words associated with them. These are only the top 20 topics out of 100 but as you can see the computer is doing a very good job at grouping like minded topics together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./visuals/lsa_topic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word2Vec Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2vec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words. Word2vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space. Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located in close proximity to one another in the space.\n",
    "\n",
    "In this project I constructed a custome Word2Vec model using tensorflow which was exciting but didnt yield many interprettable results. Using the preloaded Gensim Word2Vec library I was able to fit a model that allowed us to gain some further understanding of how the computer was vectorizing these words. \n",
    "\n",
    "Below is an example showing the use of Word2Vecs \"most_simliar\" function that allows you to find the words most closely associated, mathematically, with the word entered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./visuals/pizza_w2v.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I find this incredibly fascinating and useful from a pratical sense as a Chef or Restaurant Operator. This algorithm does a very good job or grouping semantically similar words, from reviews, together to give you a better idea how people are using these words in their reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification Modeling\n",
    "----------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was not able to get an AWS instance properly functiong for me to run my modeling on so I instead opt to simply subset out 100000 reviews from the main data set and run my models on it. Although not as comprehensive I still feel this was a large enough sample of data to make accurate predictions from. I first compared Logistic regression, Random Forest and Linear SVC models with a pipeline and gridsearch to see which performed best. Then I manually rand the logistic regression and Linear SVC models to then show metrics and confusion matrices for comparison.\n",
    "\n",
    "Based on the assumption that reviews that are 3 stars or below will hurt a restaurants overall star rating, whether they are positive in sentiment or not, I chose to create a binary classification where reviews with 3 stars or less are considered negative and reviews with 4 or 5 stars are considered positive. There was an issue of unbalanced classes slightly with a 66% to 33% split with positive and negative respectfully. To counter act this I stratified my target variable \"is_positive\" in the train/test splitting phase before modeling. Below are the two models scored by Training and Test scores, the training naturally overfits on both models due to the use of a sparse matrix with 20000+ features but the Test scores are what we are interested in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression with TFIDF Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./visuals/logreg_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We achieved a 86.45% accuracy, which means thst our model is correctly classifying, based on our parameters, if a review is positive or negative. Below is the confusion matrix as well as the feature importances(words that affect the model positively and negatively)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./visuals/logreg_metric.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./visuals/cm_logreg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./visuals/logreg_feat.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linear SVC with TFIDF vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./visuals/svc_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we achieved a slightly lower accuracy of 85.25%, I do believe that with more parameter tuning this model would end up performing better than the logistic regression model, a project for another time. Below is the confusion matrix and feature importances for this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./visuals/lsvc_metric.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./visuals/cm_lsvc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its interesting to note that the Linear SVC seems to do slighlyt better predicting true positive and true negtive reviews but has an increase in fale positives over the Logistic Regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./visuals/lsvc_feat.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature importances for both models are relatively simliar however it seems the Logisitc Regression model is more consisten in the positive words of importance. In the Linear SVC theres a bit of an outlier with \"unruly\" being classified as a positive importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work\n",
    "----------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is far from completed but I believe I have solid base to start building from to continue to use yelp reviews to positively assist restaurants and food establishments to better understand their customers wants and needs, to in turn better serve them and of course be more successful. Below are a few points I am going to continue to ecplore into the future.\n",
    "\n",
    "1. Create a new classification metric to better represent positive and negative reviews before modeling. I took a very rudimentry approach by simply splitting the reviews on 3 stars. Ideally we would use the number of positive and negative words in conjunction with the length of the review to best classify a review as positive or negative pre modeling. I wasunable to accomplish that at this point but will find a way to have a reasonable metric similar to model on later.\n",
    "2. Use cloud computing(AWS) to run models on the entire dataset. Even after cleaning the review dataset to just restautn and food reviews we still had over 3.6 million. This was far too much for my computer to handle but going forward I will continue to learn AWS to better represent the entirety of the data in a model, thus leading to better predictions.\n",
    "3. Use LSA topic modeling weights in modeling as well as using a Convolutional Neural Network with Word2vec embeddings for classification. This is a very computationally intense and diffcult process that I couldnt finish before this project was due but in my continuing education I will work to solve these problems so I can better classify positive and negative reviews and their sentiment for restaurants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
